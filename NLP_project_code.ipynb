{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 10831978,
          "sourceType": "datasetVersion",
          "datasetId": 6726299
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "NLP_project_code",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xuFoag5nRrzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df=pd.read_csv('/kaggle/input/nlp-project/Sentiment_Stock_data.csv')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-25T12:52:36.242713Z",
          "iopub.execute_input": "2025-02-25T12:52:36.243002Z",
          "iopub.status.idle": "2025-02-25T12:52:36.409479Z",
          "shell.execute_reply.started": "2025-02-25T12:52:36.242981Z",
          "shell.execute_reply": "2025-02-25T12:52:36.408821Z"
        },
        "id": "63l4qtpwRrzU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "df1, df2, df3 = np.array_split(df, 3)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-25T12:43:48.194494Z",
          "iopub.execute_input": "2025-02-25T12:43:48.194876Z",
          "iopub.status.idle": "2025-02-25T12:43:48.204662Z",
          "shell.execute_reply.started": "2025-02-25T12:43:48.194845Z",
          "shell.execute_reply": "2025-02-25T12:43:48.203764Z"
        },
        "id": "E3IozEx7RrzU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df3.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-25T12:44:01.747206Z",
          "iopub.execute_input": "2025-02-25T12:44:01.747501Z",
          "iopub.status.idle": "2025-02-25T12:44:01.752548Z",
          "shell.execute_reply.started": "2025-02-25T12:44:01.747477Z",
          "shell.execute_reply": "2025-02-25T12:44:01.751671Z"
        },
        "id": "cvGdMbsCRrzU",
        "outputId": "9bb83161-99c0-4ad8-af68-f71a8fe78d57"
      },
      "outputs": [
        {
          "execution_count": 86,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(36250, 3)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QWhJttQ2RrzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns=df.columns.str.replace(\" \",\"_\")\n",
        "df1=df.drop(columns=['Unnamed:_0'])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-25T12:52:39.793776Z",
          "iopub.execute_input": "2025-02-25T12:52:39.794098Z",
          "iopub.status.idle": "2025-02-25T12:52:39.804768Z",
          "shell.execute_reply.started": "2025-02-25T12:52:39.794075Z",
          "shell.execute_reply": "2025-02-25T12:52:39.803849Z"
        },
        "id": "VPmFUkVTRrzV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6UV7G3ycRrzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "# Fill NaN values in the 'Sentence' column with empty strings\n",
        "df1['Sentence'] = df1['Sentence'].fillna('')\n",
        "\n",
        "# Define the function to remove punctuation\n",
        "exclude = string.punctuation\n",
        "def remove_punc(text):\n",
        "    # text is now guaranteed to be a string because we filled NaN with empty strings.\n",
        "    return text.translate(str.maketrans('', '', exclude))\n",
        "\n",
        "# Apply the remove_punc function to the 'Sentence' column\n",
        "df1['Sentence'] = df1['Sentence'].apply(remove_punc)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-25T12:52:47.876067Z",
          "iopub.execute_input": "2025-02-25T12:52:47.876362Z",
          "iopub.status.idle": "2025-02-25T12:52:47.881946Z",
          "shell.execute_reply.started": "2025-02-25T12:52:47.876339Z",
          "shell.execute_reply": "2025-02-25T12:52:47.880952Z"
        },
        "id": "0xY_dWibRrzW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rDctYj62RrzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_words = {\n",
        "    \"AFAIK\": \"As Far As I Know\",\n",
        "    \"AFK\": \"Away From Keyboard\",\n",
        "    \"ASAP\": \"As Soon As Possible\",\n",
        "    \"ATK\": \"At The Keyboard\",\n",
        "    \"ATM\": \"At The Moment\",\n",
        "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
        "    \"BAK\": \"Back At Keyboard\",\n",
        "    \"BBL\": \"Be Back Later\",\n",
        "    \"BBS\": \"Be Back Soon\",\n",
        "    \"BFN\": \"Bye For Now\",\n",
        "    \"B4N\": \"Bye For Now\",\n",
        "    \"BRB\": \"Be Right Back\",\n",
        "    \"BRT\": \"Be Right There\",\n",
        "    \"BTW\": \"By The Way\",\n",
        "    \"B4\": \"Before\",\n",
        "    \"CU\": \"See You\",\n",
        "    \"CUL8R\": \"See You Later\",\n",
        "    \"CYA\": \"See You\",\n",
        "    \"FAQ\": \"Frequently Asked Questions\",\n",
        "    \"FC\": \"Fingers Crossed\",\n",
        "    \"FWIW\": \"For What It's Worth\",\n",
        "    \"FYI\": \"For Your Information\",\n",
        "    \"GAL\": \"Get A Life\",\n",
        "    \"GG\": \"Good Game\",\n",
        "    \"GN\": \"Good Night\",\n",
        "    \"GMTA\": \"Great Minds Think Alike\",\n",
        "    \"GR8\": \"Great!\",\n",
        "    \"G9\": \"Genius\",\n",
        "    \"IC\": \"I See\",\n",
        "    \"ICQ\": \"I Seek You\",\n",
        "    \"ILU\": \"I Love You\",\n",
        "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
        "    \"IMO\": \"In My Opinion\",\n",
        "    \"IOW\": \"In Other Words\",\n",
        "    \"IRL\": \"In Real Life\",\n",
        "    \"KISS\": \"Keep It Simple, Stupid\",\n",
        "    \"LDR\": \"Long Distance Relationship\",\n",
        "    \"LMAO\": \"Laugh My A** Off\",\n",
        "    \"LOL\": \"Laughing Out Loud\",\n",
        "    \"LTNS\": \"Long Time No See\",\n",
        "    \"L8R\": \"Later\",\n",
        "    \"MTE\": \"My Thoughts Exactly\",\n",
        "    \"M8\": \"Mate\",\n",
        "    \"NRN\": \"No Reply Necessary\",\n",
        "    \"OIC\": \"Oh I See\",\n",
        "    \"PITA\": \"Pain In The A**\",\n",
        "    \"PRT\": \"Party\",\n",
        "    \"PRW\": \"Parents Are Watching\",\n",
        "    \"QPSA\": \"Que Pasa?\",\n",
        "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
        "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
        "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A** Off\",\n",
        "    \"SK8\": \"Skate\",\n",
        "    \"STATS\": \"Your Sex and Age\",\n",
        "    \"ASL\": \"Age, Sex, Location\",\n",
        "    \"THX\": \"Thank You\",\n",
        "    \"TTFN\": \"Ta-Ta For Now!\",\n",
        "    \"TTYL\": \"Talk To You Later\",\n",
        "    \"U\": \"You\",\n",
        "    \"U2\": \"You Too\",\n",
        "    \"U4E\": \"Yours For Ever\",\n",
        "    \"WB\": \"Welcome Back\",\n",
        "    \"WTF\": \"What The F***\",\n",
        "    \"WTG\": \"Way To Go!\",\n",
        "    \"WUF\": \"Where Are You From?\",\n",
        "    \"W8\": \"Wait...\",\n",
        "    \"7K\": \"Sick:-D Laughter\",\n",
        "    \"TFW\": \"That Feeling When\",\n",
        "    \"MFW\": \"My Face When\",\n",
        "    \"MRW\": \"My Reaction When\",\n",
        "    \"IFYP\": \"I Feel Your Pain\",\n",
        "    \"LOL\": \"Laughing Out Loud\",\n",
        "    \"TNTL\": \"Trying Not To Laugh\",\n",
        "    \"JK\": \"Just Kidding\",\n",
        "    \"IDC\": \"I Don't Care\",\n",
        "    \"ILY\": \"I Love You\",\n",
        "    \"IMU\": \"I Miss You\",\n",
        "    \"ADIH\": \"Another Day In Hell\",\n",
        "    \"ZZZ\": \"Sleeping, Bored, Tired\",\n",
        "    \"WYWH\": \"Wish You Were Here\",\n",
        "    \"TIME\": \"Tears In My Eyes\",\n",
        "    \"BAE\": \"Before Anyone Else\",\n",
        "    \"FIMH\": \"Forever In My Heart\",\n",
        "    \"BSAAW\": \"Big Smile And A Wink\",\n",
        "    \"BWL\": \"Bursting With Laughter\",\n",
        "    \"BFF\": \"Best Friends Forever\",\n",
        "    \"CSL\": \"Can't Stop Laughing\"\n",
        "}\n",
        "\n",
        "def chat_conversion(text):\n",
        "  new_text=[]\n",
        "  for w in text.split():\n",
        "    if w.upper() in chat_words:\n",
        "      new_text.append(chat_words[w.upper()])\n",
        "    else:\n",
        "      new_text.append(w)\n",
        "  return ' '.join(new_text)\n",
        "\n",
        "df1['Sentence']=df1['Sentence'].apply(chat_conversion)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-25T12:52:50.212686Z",
          "iopub.execute_input": "2025-02-25T12:52:50.212998Z",
          "iopub.status.idle": "2025-02-25T12:52:50.223779Z",
          "shell.execute_reply.started": "2025-02-25T12:52:50.212973Z",
          "shell.execute_reply": "2025-02-25T12:52:50.222774Z"
        },
        "id": "2k5nuj_yRrzW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u1zOJ4qTRrzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "df1['Sentence']=df1['Sentence'].apply(lambda x:str(TextBlob(x).correct()))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-25T12:52:53.643111Z",
          "iopub.execute_input": "2025-02-25T12:52:53.643412Z",
          "iopub.status.idle": "2025-02-25T12:52:56.946579Z",
          "shell.execute_reply.started": "2025-02-25T12:52:53.643387Z",
          "shell.execute_reply": "2025-02-25T12:52:56.945824Z"
        },
        "id": "PB73v3wyRrzX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NvA8ZvXeRrzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "def remove_stopwords(text):\n",
        "  new_text=[]\n",
        "  for word in text.split():\n",
        "    if word in stopwords.words('english'):\n",
        "      new_text.append('')\n",
        "    else:\n",
        "      new_text.append(word)\n",
        "  x=new_text[:]\n",
        "  new_text.clear()\n",
        "  return \" \".join(x)\n",
        "\n",
        "df1['Sentence']=df1['Sentence'].apply(remove_stopwords)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-25T12:54:07.428463Z",
          "iopub.execute_input": "2025-02-25T12:54:07.428837Z",
          "iopub.status.idle": "2025-02-25T12:54:07.471639Z",
          "shell.execute_reply.started": "2025-02-25T12:54:07.428807Z",
          "shell.execute_reply": "2025-02-25T12:54:07.470785Z"
        },
        "id": "3I8c5r0PRrzX",
        "outputId": "04fe4d48-2f78-4765-a45b-62834d6b0428"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GAnpe8qqRrzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uS4xiclzRrzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "df1['lemmas']=df1['Sentence'].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x)]))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-25T12:54:10.935627Z",
          "iopub.execute_input": "2025-02-25T12:54:10.935939Z",
          "iopub.status.idle": "2025-02-25T12:54:11.630526Z",
          "shell.execute_reply.started": "2025-02-25T12:54:10.935912Z",
          "shell.execute_reply": "2025-02-25T12:54:11.629795Z"
        },
        "id": "MDOD5clPRrzX",
        "outputId": "4dfcc983-6604-4dcb-ce8e-9f0a7376585c"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n  warnings.warn(Warnings.W111)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5UA1xaRjRrzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9uBe_U0sRrzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gmeqwSllRrzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_q8UFd24RrzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "from spacy.pipeline import EntityRuler\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "# Load the SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Custom tokenizer to split \"EUR123\" into \"EUR\" and \"123\"\n",
        "def custom_tokenizer(nlp):\n",
        "    infix_re = re.compile(r\"(?<=[A-Za-z])(?=\\d)|(?<=\\d)(?=[A-Za-z])\")\n",
        "    return Tokenizer(nlp.vocab, infix_finditer=infix_re.finditer)\n",
        "\n",
        "nlp.tokenizer = custom_tokenizer(nlp)\n",
        "\n",
        "# Add EntityRuler for recognizing \"EUR\" followed by numbers as MONEY\n",
        "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "patterns = [{\"label\": \"MONEY\", \"pattern\": [{\"LOWER\": \"eur\"}, {\"IS_DIGIT\": True}]}]\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "\n",
        "# Function to extract entities\n",
        "def extract_entities(text):\n",
        "    doc = nlp(str(text))  # Convert to string to avoid errors\n",
        "\n",
        "    orgs = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
        "    products = [ent.text for ent in doc.ents if ent.label_ == \"PRODUCT\"]\n",
        "    money = [ent.text for ent in doc.ents if ent.label_ == \"MONEY\"]\n",
        "\n",
        "    return {\"ORG\": \", \".join(orgs), \"PRODUCT\": \", \".join(products), \"MONEY\": \", \".join(money)}\n",
        "\n",
        "# Apply entity extraction\n",
        "df1[\"Entities\"] = df1[\"Sentence\"].apply(lambda x: extract_entities(x))\n",
        "\n",
        "df_filtered = df_entities[df_entities[\"ORG\"] != \"\"]\n",
        "\n",
        "# Display the filtered DataFrame\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-25T12:54:15.52879Z",
          "iopub.execute_input": "2025-02-25T12:54:15.529092Z",
          "iopub.status.idle": "2025-02-25T12:54:16.236029Z",
          "shell.execute_reply.started": "2025-02-25T12:54:15.529069Z",
          "shell.execute_reply": "2025-02-25T12:54:16.23503Z"
        },
        "id": "psroUlHkRrzX",
        "outputId": "3f5f2344-ec02-4b43-cbed-d15dd55176c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n  warnings.warn(Warnings.W111)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TvSt0HaKRrzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BErY3671RrzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# Load sentiment analysis model\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "\n",
        "# Function to analyze sentiment of the context\n",
        "def analyze_sentiment(text):\n",
        "    if not text.strip():  # Skip empty or whitespace-only texts\n",
        "        return None, None\n",
        "    result = sentiment_pipeline(text)[0]\n",
        "    return result[\"label\"].lower(), result[\"score\"]  # Ensure lowercase labels\n",
        "\n",
        "df1[\"lemmas\"] = df1[\"lemmas\"].fillna(\"\")  # Replace NaNs with empty strings\n",
        "df1[[\"Sentiments\", \"Confidence\"]] = df1[\"lemmas\"].apply(lambda x: pd.Series(analyze_sentiment(x)))\n",
        "\n",
        "# Map sentiment labels to numeric values\n",
        "df1[\"Sentiments\"] = df1[\"Sentiments\"].map({\"positive\": 1, \"neutral\": 0, \"negative\": -1})\n",
        "\n",
        "df1 = df1.dropna(subset=[\"Sentiments\", \"Confidence\"])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-25T12:25:21.027362Z",
          "iopub.execute_input": "2025-02-25T12:25:21.02773Z",
          "iopub.status.idle": "2025-02-25T12:25:22.6452Z",
          "shell.execute_reply.started": "2025-02-25T12:25:21.027698Z",
          "shell.execute_reply": "2025-02-25T12:25:22.644415Z"
        },
        "id": "ZIqa7j6dRrzX",
        "outputId": "c528f94e-9dd7-4afe-b769-2bf7accac7d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nDevice set to use cuda:0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Sentiments'].unique()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-25T12:25:34.608925Z",
          "iopub.execute_input": "2025-02-25T12:25:34.609213Z",
          "iopub.status.idle": "2025-02-25T12:25:34.614774Z",
          "shell.execute_reply.started": "2025-02-25T12:25:34.609191Z",
          "shell.execute_reply": "2025-02-25T12:25:34.613907Z"
        },
        "id": "RHhJDu7ZRrzY",
        "outputId": "561facc9-93e2-40e4-e89c-6fcbf6a9e916"
      },
      "outputs": [
        {
          "execution_count": 64,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([0, 1])"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from transformers import pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load Sentiment Analysis Model\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "\n",
        "# Prepare ML Data\n",
        "X = df1[[\"Sentiments\", \"Confidence\"]]\n",
        "y = df1[\"Sentiment\"]\n",
        "\n",
        "# Train Model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict Stock Categories\n",
        "df1[\"Predicted_Category\"] = model.predict(X)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-25T12:25:41.701854Z",
          "iopub.execute_input": "2025-02-25T12:25:41.702165Z",
          "iopub.status.idle": "2025-02-25T12:25:42.466581Z",
          "shell.execute_reply.started": "2025-02-25T12:25:41.702136Z",
          "shell.execute_reply": "2025-02-25T12:25:42.465689Z"
        },
        "id": "njx1g0eTRrzY",
        "outputId": "f3d37e96-0677-4eb8-f79b-e93472cf0d2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nDevice set to use cuda:0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df1[\"Predicted_Category\"].unique()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-25T12:26:22.409964Z",
          "iopub.execute_input": "2025-02-25T12:26:22.410287Z",
          "iopub.status.idle": "2025-02-25T12:26:22.415979Z",
          "shell.execute_reply.started": "2025-02-25T12:26:22.410259Z",
          "shell.execute_reply": "2025-02-25T12:26:22.414976Z"
        },
        "id": "tHHN0r81RrzY",
        "outputId": "de1e8308-ee45-4b63-fd32-a5e882fcf383"
      },
      "outputs": [
        {
          "execution_count": 68,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([1, 0])"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df1[\"ORG\"] = df1[\"Entities\"].apply(lambda x: x.get(\"ORG\", None))\n",
        "df1[\"PRODUCT\"] = df1[\"Entities\"].apply(lambda x: x.get(\"PRODUCT\", None))\n",
        "df1[\"MONEY\"] = df1[\"Entities\"].apply(lambda x: x.get(\"MONEY\", None))\n",
        "df1.head(10)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-25T12:52:11.336852Z",
          "iopub.execute_input": "2025-02-25T12:52:11.337145Z",
          "iopub.status.idle": "2025-02-25T12:52:11.412101Z",
          "shell.execute_reply.started": "2025-02-25T12:52:11.337123Z",
          "shell.execute_reply": "2025-02-25T12:52:11.410808Z"
        },
        "id": "cN5MCKeORrzY",
        "outputId": "586646e0-7050-4db9-f490-defcb69042fb"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Entities'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-04b7b4d13e5d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ORG\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Entities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ORG\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PRODUCT\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Entities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PRODUCT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MONEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Entities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MONEY\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Entities'"
          ],
          "ename": "KeyError",
          "evalue": "'Entities'",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get dependency parsing for each word in a column\n",
        "def dependency_parse(text):\n",
        "    if pd.isna(text):  # Handle None or NaN values\n",
        "        return None\n",
        "    doc = nlp(str(text))\n",
        "    return [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "\n",
        "# Apply dependency parsing to each column\n",
        "print(df1[\"ORG\"].apply(dependency_parse))\n",
        "print(df1[\"PRODUCT\"].apply(dependency_parse))\n",
        "print(df1[\"MONEY\"].apply(dependency_parse))"
      ],
      "metadata": {
        "trusted": true,
        "id": "63PyI6PfRrzY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from spacy import displacy\n",
        "\n",
        "# Function to display dependency tree\n",
        "def visualize_dependency(text):\n",
        "    if pd.isna(text):  # Handle None or NaN values\n",
        "        return None\n",
        "    doc = nlp(str(text))\n",
        "    displacy.render(doc, style=\"dep\", jupyter=True)  # Show visualization\n",
        "\n",
        "# Example: Show dependency tree for \"ORG\" column (first row with non-null value)\n",
        "for text in df1[\"ORG\"].dropna():\n",
        "    visualize_dependency(text)"
      ],
      "metadata": {
        "trusted": true,
        "id": "0CvrlFObRrzY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "xx = df1.groupby(\"Predicted_Category\")[[\"ORG\", \"PRODUCT\", \"MONEY\"]].agg(list).reset_index()\n",
        "xx"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-25T12:51:56.744385Z",
          "iopub.execute_input": "2025-02-25T12:51:56.744739Z",
          "iopub.status.idle": "2025-02-25T12:51:56.782731Z",
          "shell.execute_reply.started": "2025-02-25T12:51:56.744711Z",
          "shell.execute_reply": "2025-02-25T12:51:56.781452Z"
        },
        "id": "c-66isEcRrzY",
        "outputId": "8e9499f3-b922-4946-cfda-36d7605e7d7f"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-e8a014b04e58>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted_Category\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ORG\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PRODUCT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MONEY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9181\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to supply one of 'by' and 'level'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9183\u001b[0;31m         return DataFrameGroupBy(\n\u001b[0m\u001b[1;32m   9184\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9185\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgrouper\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Predicted_Category'"
          ],
          "ename": "KeyError",
          "evalue": "'Predicted_Category'",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "xx.to_csv(\"df1.csv\", index=False)  # Saves without the index column"
      ],
      "metadata": {
        "trusted": true,
        "id": "TzFkyPW7RrzY"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}